{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses of the Hofree et al. original datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the preprocessed data in Matlab format\n",
    "For the current analyses, all the required preprocessed data are already in the folder \"data.\"\n",
    "\n",
    "If you want to generate them from the original Hofree et al. dataset, you have to use the script \"Matlab2Python.m\" in the \"tools\" folder. The original dataset from Hofree et al. with their Matlab code is available on the UCSD's [Network Based Stratification](http://chianti.ucsd.edu/~mhofree/wordpress/?page_id=26) webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "dataFolder='data/'\n",
    "\n",
    "# Patients' somatic mutation profiles\n",
    "data = loadmat(dataFolder+'somatic_data_OV.mat')\n",
    "# Adjacency matrix\n",
    "network = loadmat(dataFolder+'adj_mat.mat')\n",
    "# Correspondance between matrices rows number and entrez id\n",
    "entrez_to_idmat = loadmat(dataFolder+'entrez_to_idmat.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check preprocessed data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.keys()\n",
    "len(data['gene_id_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutations=data['gene_indiv_mat']\n",
    "mutations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print network.keys()\n",
    "net=network['adj_mat']\n",
    "net.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entrez_to_idmat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(entrez_to_idmat['keymat'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all the ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys=[x[0] for x in entrez_to_idmat['keymat'][0]]\n",
    "ids=[x[0][0] for x in entrez_to_idmat['entrezid'][0]]\n",
    "genes = [x[0] for x in data['gene_id_all']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Ensembl ID:\", keys[0]\n",
    "print \"Entrez ID:\", ids[0]\n",
    "print \"Check on NCBI: http://www.ncbi.nlm.nih.gov/gene/%i\" % ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract indexes of the genes in the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "l=[]\n",
    "subnet=[]\n",
    "good=[]\n",
    "bad=[]\n",
    "for j,g in enumerate(genes):\n",
    "    try:\n",
    "        i=ids.index(g)\n",
    "        subnet.append(i)\n",
    "        good.append(j)\n",
    "    except:\n",
    "        i=np.nan\n",
    "        bad.append(j)\n",
    "    l.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"All genes:\",len(l)\n",
    "print \"Referenced in the PPI:\",len(good)\n",
    "print \"On their own:\",len(bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the submatrices of references genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnet=net[subnet][:,subnet]\n",
    "nnet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmut=mutations[:,good]\n",
    "nmut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-padding of the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnnet=np.bmat([[np.matrix(nnet.todense()), np.matrix(np.zeros([nnet.shape[0],len(bad)]))], [np.matrix(np.zeros([len(bad),nnet.shape[0]])), np.matrix(np.diagflat(np.zeros(len(bad))))]])\n",
    "nnmut=mutations[:,good+bad]\n",
    "symbols=data['gene_id_symbol'][good+bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Network size:\",nnnet.shape\n",
    "print \"Mutation size:\",nnmut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "degree=np.squeeze(np.array(nnnet.sum(axis=0)))\n",
    "figure(1,figsize=(16,10))\n",
    "plot(degree)\n",
    "ylabel(\"Degree (number of neighboors in the PPI)\")\n",
    "xlabel(\"Genes (keys)\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering according to Hofree et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing network influence score [warning: very long!]\n",
    "For more details, see: Vandin, F., Upfal, E., & Raphael, B. J. (2011). Algorithms for Detecting Significantly Mutated Pathways in Cancer. Journal of Computational Biology, 18(3), 507â€“522. http://doi.org/10.1089/cmb.2010.0265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove genes on their own\n",
    "nnnetFiltered=nnnet[degree>0,:][:,degree>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "computeInfluence=False\n",
    "\n",
    "if computeInfluence:\n",
    "    from scipy.io import savemat\n",
    "    from IPython.html.widgets import FloatProgress\n",
    "    from IPython.display import display \n",
    "\n",
    "    diffusionFactor=0.8\n",
    "    influence=np.zeros(nnnetFiltered.shape)\n",
    "    influencers=np.zeros(nnnetFiltered.shape)\n",
    "    f = FloatProgress(min=0, max=nnnetFiltered.shape[0])\n",
    "    display(f)\n",
    "\n",
    "    for i in range(nnnetFiltered.shape[0]):\n",
    "        f.value = i\n",
    "        tmp=np.array(nnnetFiltered.sum(axis=0))\n",
    "        tmp[tmp==0]=1\n",
    "        A=nnnetFiltered*np.diagflat(1./tmp)\n",
    "        mutationProfile=np.zeros(nnnetFiltered.shape[0])\n",
    "        mutationProfile[i]=1\n",
    "        X1=mutationProfile\n",
    "        X2=diffusionFactor*X1*A+(1-diffusionFactor)*mutationProfile\n",
    "        while norm(X2-X1)>10e-6:\n",
    "            X1=X2\n",
    "            X2=diffusionFactor*X1*A+(1-diffusionFactor)*mutationProfile\n",
    "        influence[i,:]=np.squeeze(X2)\n",
    "    \n",
    "    #Save the raw influence distance matrix (heavy!)\n",
    "    savemat(dataFolder+'influenceDistance.mat',{'influence':influence, 'diffusionFactor':diffusionFactor})\n",
    "    #Save the sparse influence distance by merging with PPI\n",
    "    from scipy.sparse import lil_matrix\n",
    "    PPI_influence=lil_matrix(np.multiply(np.min(np.dstack((influence, influence.T)),axis=2),np.array(nnnetFiltered)))\n",
    "    savemat(dataFolder+'PPI_influence.mat',{'PPI_influence':PPI_influence, 'diffusionFactor':diffusionFactor}, do_compression=True)\n",
    "else:\n",
    "    influence_data = loadmat(dataFolder+'PPI_influence.mat')\n",
    "    PPI_influence=influence_data['PPI_influence']\n",
    "    diffusionFactor=influence_data['diffusionFactor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping only the connections with the best influencers\n",
    "\"The degree to which local network topology versus global network topology constrains W is determined by the number of nearest neighbors. We experimented with neighbor counts ranging from 5 to 50 to include in the nearest network, and we observed only small changes in outcome (data not shown). For the work presented in this manuscript, the 11 most influential neighbors of each gene in the network as determined by network influence distance were used.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PPIneighboorsMax=11\n",
    "influenceMat=PPI_influence.todense()\n",
    "newnet=np.zeros(nnnetFiltered.shape)\n",
    "for i in range(nnnetFiltered.shape[0]):\n",
    "    bestInfluencers=np.argsort(influenceMat[i,:])[:,-PPIneighboorsMax:]\n",
    "    newnet[i,bestInfluencers]=np.squeeze(np.array(nnnetFiltered[i,bestInfluencers]))\n",
    "\n",
    "newnet=np.min(np.dstack((newnet, newnet.T)),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(18,9))\n",
    "subplot(121)\n",
    "imshow(nnnetFiltered)\n",
    "set_cmap('Greys')\n",
    "title(\"Original adjacency\")\n",
    "subplot(122)\n",
    "imshow(newnet)\n",
    "title(\"With only the \"+str(PPIneighboorsMax)+\" best influencers\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notAlone=newnet.sum(axis=1)>0\n",
    "print nnnet.shape, nnnet[degree>0,:][:,degree>0].shape, newnet[notAlone,:][:,notAlone].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnnetFiltered=nnnet[degree>0,:][:,degree>0]\n",
    "filteredGenes=degree==0\n",
    "filteredGenes[filteredGenes==False]=newnet.sum(axis=1)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutationsMin=10\n",
    "filteredPatients=nnmut.sum(axis=1)<mutationsMin\n",
    "print \"Removing %i genes filtered with the %i influencers criterion\" % (filteredGenes.sum(), PPIneighboorsMax)\n",
    "print \"Removing %i patients with less than %i mutations\" % (filteredPatients.sum(),mutationsMin)\n",
    "notAlone=newnet.sum(axis=1)>0\n",
    "nnnetFiltered=newnet[notAlone,:][:,notAlone]\n",
    "nnmutFiltered=nnmut[filteredPatients==False,:]\n",
    "nnmutFiltered=nnmutFiltered[:,filteredGenes==False]\n",
    "print \"New adjacency matrix:\",nnnetFiltered.shape\n",
    "print \"New mutation profile matrix:\",nnmutFiltered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion of the mutation profiles according to the PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutationProfileDiffusion(mutationProfile,PPIAdjacencyMatrix,diffusionFactor):\n",
    "    PPIAdjacencyMatrix=PPIAdjacencyMatrix+np.diagflat(np.ones(PPIAdjacencyMatrix.shape[0]))\n",
    "    tmp=np.array(PPIAdjacencyMatrix.sum(axis=0))\n",
    "    A=np.dot(PPIAdjacencyMatrix,np.diagflat(1./tmp))\n",
    "    X1=mutationProfile\n",
    "    X2=diffusionFactor*np.dot(X1,A)+(1-diffusionFactor)*mutationProfile\n",
    "    while norm(X2-X1)>10e-6:\n",
    "        X1=X2\n",
    "        X2=diffusionFactor*np.dot(X1,A)+(1-diffusionFactor)*mutationProfile\n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnmutDiffused=mutationProfileDiffusion(nnmutFiltered,nnnetFiltered,0.8)\n",
    "nnmutDiffused[np.isnan(nnmutDiffused)]=0\n",
    "plt.figure(1,figsize=(16,10))\n",
    "subplot(211)\n",
    "plot(np.squeeze(np.asarray(nnmutFiltered[0,:])))\n",
    "title(\"Original mutation profile\")\n",
    "subplot(212)\n",
    "plot(np.squeeze(np.asarray(nnmutDiffused[0,:])))\n",
    "title(\"Diffused mutation profile\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(1,figsize=(16,5))\n",
    "subplot(311)\n",
    "imshow(nnmutFiltered)\n",
    "title(\"Original mutation profile\")\n",
    "subplot(312)\n",
    "imshow(nnmutDiffused)\n",
    "title(\"Diffused mutation profile\")\n",
    "subplot(313)\n",
    "hist(np.array(np.squeeze(nnmutDiffused.reshape((1,-1)))).T, 50, normed=1, histtype='stepfilled')\n",
    "title(\"Weigths histogram after diffusion\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix (NMF) decomposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import ProjectedGradientNMF\n",
    "model = ProjectedGradientNMF(n_components=3, init='random', random_state=0)\n",
    "\n",
    "model.fit(np.matrix(nnmutFiltered))\n",
    "sklearnComp=model.components_\n",
    "sklearnStrat=np.argmax(model.transform(np.matrix(nnmutFiltered)),axis=1)\n",
    "\n",
    "model.fit(np.matrix(nnmutDiffused))\n",
    "sklearnCompDiff=model.components_\n",
    "sklearnStratDiff=np.argmax(model.transform(np.matrix(nnmutDiffused)),axis=1)\n",
    "\n",
    "plt.figure(1,figsize=(16,10))\n",
    "subplot(211)\n",
    "plot(sklearnComp.T/sklearnComp.max())\n",
    "ylabel(\"Weight\")\n",
    "xlabel(\"Genes\")\n",
    "title(\"NMF decomposition on raw mutation profiles\")\n",
    "subplot(212)\n",
    "plot(sklearnCompDiff.T/sklearnCompDiff.max())\n",
    "ylabel(\"Weight\")\n",
    "xlabel(\"Genes\")\n",
    "title(\"NMF decomposition on diffused mutation profiles\")\n",
    "legend({'Component 1','Component 2','Component 3'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNMF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reuse scikit-learn functions\n",
    "import scipy.sparse as sp\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.extmath import randomized_svd, safe_sparse_dot\n",
    "\n",
    "def check_non_negative(X, whom):\n",
    "    X = X.data if sp.issparse(X) else X\n",
    "    if (X < 0).any():\n",
    "        raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
    "\n",
    "def _sparseness(x):\n",
    "    \"\"\"Hoyer's measure of sparsity for a vector\"\"\"\n",
    "    sqrt_n = np.sqrt(len(x))\n",
    "    return (sqrt_n - np.linalg.norm(x, 1) / norm(x)) / (sqrt_n - 1)\n",
    "\n",
    "def safe_vstack(Xs):\n",
    "    if any(sp.issparse(X) for X in Xs):\n",
    "        return sp.vstack(Xs)\n",
    "    else:\n",
    "        return np.vstack(Xs)\n",
    "\n",
    "def NBS_init(X,n_components,init=None):\n",
    "        n_samples, n_features = X.shape\n",
    "        if init is None:\n",
    "            if n_components < n_features:\n",
    "                init = 'nndsvd'\n",
    "            else:\n",
    "                init = 'random'\n",
    "\n",
    "\n",
    "        if init == 'nndsvd':\n",
    "            W, H = _initialize_nmf(X, n_components)\n",
    "        elif init == \"random\":\n",
    "            rng = check_random_state(random_state)\n",
    "            W = rng.randn(n_samples, n_components)\n",
    "            # we do not write np.abs(W, out=W) to stay compatible with\n",
    "            # numpy 1.5 and earlier where the 'out' keyword is not\n",
    "            # supported as a kwarg on ufuncs\n",
    "            np.abs(W, W)\n",
    "            H = rng.randn(n_components, n_features)\n",
    "            np.abs(H, H)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Invalid init parameter: got %r instead of one of %r' %\n",
    "                (init, (None, 'nndsvd', 'nndsvda', 'nndsvdar', 'random')))\n",
    "        return W, H\n",
    "\n",
    "def _initialize_nmf(X, n_components, variant=None, eps=1e-6,\n",
    "                    random_state=None):\n",
    "    \"\"\"NNDSVD algorithm for NMF initialization.\n",
    "\n",
    "    Computes a good initial guess for the non-negative\n",
    "    rank k matrix approximation for X: X = WH\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : array, [n_samples, n_features]\n",
    "        The data matrix to be decomposed.\n",
    "\n",
    "    n_components : array, [n_components, n_features]\n",
    "        The number of components desired in the approximation.\n",
    "\n",
    "    variant : None | 'a' | 'ar'\n",
    "        The variant of the NNDSVD algorithm.\n",
    "        Accepts None, 'a', 'ar'\n",
    "        None: leaves the zero entries as zero\n",
    "        'a': Fills the zero entries with the average of X\n",
    "        'ar': Fills the zero entries with standard normal random variates.\n",
    "        Default: None\n",
    "\n",
    "    eps: float\n",
    "        Truncate all values less then this in output to zero.\n",
    "\n",
    "    random_state : numpy.RandomState | int, optional\n",
    "        The generator used to fill in the zeros, when using variant='ar'\n",
    "        Default: numpy.random\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    (W, H) :\n",
    "        Initial guesses for solving X ~= WH such that\n",
    "        the number of columns in W is n_components.\n",
    "\n",
    "    Remarks\n",
    "    -------\n",
    "\n",
    "    This implements the algorithm described in\n",
    "    C. Boutsidis, E. Gallopoulos: SVD based\n",
    "    initialization: A head start for nonnegative\n",
    "    matrix factorization - Pattern Recognition, 2008\n",
    "\n",
    "    http://tinyurl.com/nndsvd\n",
    "    \"\"\"\n",
    "    check_non_negative(X, \"NMF initialization\")\n",
    "    if variant not in (None, 'a', 'ar'):\n",
    "        raise ValueError(\"Invalid variant name\")\n",
    "\n",
    "    U, S, V = randomized_svd(X, n_components)\n",
    "    W, H = np.zeros(U.shape), np.zeros(V.shape)\n",
    "\n",
    "    # The leading singular triplet is non-negative\n",
    "    # so it can be used as is for initialization.\n",
    "    W[:, 0] = np.sqrt(S[0]) * np.abs(U[:, 0])\n",
    "    H[0, :] = np.sqrt(S[0]) * np.abs(V[0, :])\n",
    "\n",
    "    for j in range(1, n_components):\n",
    "        x, y = U[:, j], V[j, :]\n",
    "\n",
    "        # extract positive and negative parts of column vectors\n",
    "        x_p, y_p = np.maximum(x, 0), np.maximum(y, 0)\n",
    "        x_n, y_n = np.abs(np.minimum(x, 0)), np.abs(np.minimum(y, 0))\n",
    "\n",
    "        # and their norms\n",
    "        x_p_nrm, y_p_nrm = norm(x_p), norm(y_p)\n",
    "        x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)\n",
    "\n",
    "        m_p, m_n = x_p_nrm * y_p_nrm, x_n_nrm * y_n_nrm\n",
    "\n",
    "        # choose update\n",
    "        if m_p > m_n:\n",
    "            u = x_p / x_p_nrm\n",
    "            v = y_p / y_p_nrm\n",
    "            sigma = m_p\n",
    "        else:\n",
    "            u = x_n / x_n_nrm\n",
    "            v = y_n / y_n_nrm\n",
    "            sigma = m_n\n",
    "\n",
    "        lbd = np.sqrt(S[j] * sigma)\n",
    "        W[:, j] = lbd * u\n",
    "        H[j, :] = lbd * v\n",
    "\n",
    "    W[W < eps] = 0\n",
    "    H[H < eps] = 0\n",
    "\n",
    "    if variant == \"a\":\n",
    "        avg = X.mean()\n",
    "        W[W == 0] = avg\n",
    "        H[H == 0] = avg\n",
    "    elif variant == \"ar\":\n",
    "        random_state = check_random_state(random_state)\n",
    "        avg = X.mean()\n",
    "        W[W == 0] = abs(avg * random_state.randn(len(W[W == 0])) / 100)\n",
    "        H[H == 0] = abs(avg * random_state.randn(len(H[H == 0])) / 100)\n",
    "\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted version of the NMF function to integrate graph-regularization\n",
    "#\n",
    "# See:\n",
    "# https://github.com/luispedro/milk/blob/master/milk/unsupervised/nnmf/lee_seung.py\n",
    "# https://www.researchgate.net/profile/Zhigang_Luo/publication/258350768_Limited-memory_fast_gradient_descent_method_for_graph_regularized_nonnegative_matrix_factorization/links/0c9605282f7f611648000000.pdf\n",
    "from sklearn.utils.validation import check_array\n",
    "\n",
    "def GNMF(X,L,lambd=0,n_components=None,tol=1e-4,max_iter=100,verbose=False):      \n",
    "        X = check_array(X)\n",
    "        check_non_negative(X, \"NMF.fit\")\n",
    "        n_samples, n_features = X.shape\n",
    "  \n",
    "        if not n_components:\n",
    "            n_components = n_features\n",
    "        else:\n",
    "            n_components = n_components\n",
    "  \n",
    "        #W, H = NBS_init(X,n_components)\n",
    "        W = np.random.normal(0,1,(n_samples,n_components))**2\n",
    "        H = np.random.normal(0,1,(n_components,n_features))**2\n",
    "        \n",
    "        reconstruction_err_ = norm(X - np.dot(W, H))\n",
    "        eps=1e-4#spacing(1) #10e-14\n",
    "        Lp = (abs(L)+L)/2\n",
    "        Lm = (abs(L)-L)/2\n",
    "       \n",
    "        for n_iter in range(1, max_iter + 1):\n",
    "            if verbose:\n",
    "                print \"Iteration =\", n_iter,\"/\",max_iter, \"â€” Error =\", reconstruction_err_,\"/\",tol\n",
    "            \n",
    "            h1=lambd*np.dot(H,Lm)+np.dot(W.T,(X+eps)/(np.dot(W,H)+eps))\n",
    "            h2=lambd*np.dot(H,Lp)+np.dot(W.T,np.ones(shape(X)))\n",
    "            H = multiply(H,(h1+eps)/(h2+eps))\n",
    "            H[H<=0]=eps\n",
    "            H[np.isnan(H)]=eps\n",
    "            \n",
    "            w1=np.dot((X+eps)/(np.dot(W,H)+eps),H.T)\n",
    "            w2=np.dot(np.ones(shape(X)),H.T)\n",
    "            W = multiply(W,(w1+eps)/(w2+eps))\n",
    "            W[H<=0]=eps\n",
    "            W[np.isnan(W)]=eps            \n",
    "            \n",
    "            if not sp.issparse(X):\n",
    "                if reconstruction_err_ > norm(X - np.dot(W, H)):\n",
    "                    H=(1-eps)*H+eps*np.random.normal(0,1,(n_components,n_features))**2\n",
    "                    W=(1-eps)*W+eps*np.random.normal(0,1,(n_samples,n_components))**2\n",
    "                reconstruction_err_ = norm(X - np.dot(W, H))\n",
    "            else:\n",
    "                norm2X = np.sum(X.data ** 2)  # Ok because X is CSR\n",
    "                normWHT = np.trace(np.dot(np.dot(H.T, np.dot(W.T, W)), H))\n",
    "                cross_prod = np.trace(np.dot((X * H.T).T, W))\n",
    "                reconstruction_err_ = sqrt(norm2X + normWHT - 2. * cross_prod)\n",
    "                    \n",
    "            if reconstruction_err_<tol:\n",
    "                warnings.warn(\"Tolerance error reached during fit\")\n",
    "                break\n",
    "            \n",
    "            if numpy.isnan(W).any() or numpy.isnan(H).any():\n",
    "                warnings.warn(\"NaN values at \"+ str(n_iter)+\" Error=\"+str(reconstruction_err_))\n",
    "                break\n",
    "                              \n",
    "            if n_iter == max_iter:\n",
    "                warnings.warn(\"Iteration limit reached during fit\")\n",
    "  \n",
    "        return np.squeeze(np.asarray(W)), np.squeeze(np.asarray(H)), reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WNMF, stratipyCompGNMF, reconstruction_err_ = GNMF(np.matrix(nnmutFiltered),np.matrix(nnnetFiltered),0.,n_components=3,tol=1e-3)\n",
    "WNMFDiff, stratipyCompGNMFDiff, reconstruction_err_Diff = GNMF(np.matrix(nnmutDiffused),np.matrix(nnnetFiltered),0.,n_components=3,tol=1e-3)\n",
    "W, stratipyCompG, reconstruction_err_ = GNMF(np.matrix(nnmutFiltered),np.matrix(nnnetFiltered),0.8,n_components=3,tol=1e-3)\n",
    "WDiff, stratipyCompGDiff, reconstruction_err_Diff = GNMF(np.matrix(nnmutDiffused),np.matrix(nnnetFiltered),0.8,n_components=3,tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,10))\n",
    "subplot(411)\n",
    "plot(stratipyCompGNMF.T/stratipyCompGNMF.max())\n",
    "ylabel(\"Weight\")\n",
    "xlabel(\"Genes\")\n",
    "title(\"NMF decomposition on raw mutation profiles\")\n",
    "subplot(412)\n",
    "plot(stratipyCompGNMFDiff.T/stratipyCompGNMFDiff.max())\n",
    "ylabel(\"Weight\")\n",
    "xlabel(\"Genes\")\n",
    "title(\"NMF decomposition on diffused mutation profiles\")\n",
    "subplot(413)\n",
    "plot(stratipyCompG.T/stratipyCompG.max())\n",
    "ylabel(\"Weight\")\n",
    "xlabel(\"Genes\")\n",
    "title(\"GNMF decomposition on raw mutation profiles\")\n",
    "subplot(414)\n",
    "plot(stratipyCompGDiff.T/stratipyCompGDiff.max())\n",
    "ylabel(\"Weight\")\n",
    "xlabel(\"Genes\")\n",
    "title(\"GNMF decomposition on diffused mutation profiles\")\n",
    "legend({'Component 1','Component 2','Component 3'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,5))\n",
    "Stratification=np.argmax(stratipyCompGDiff,axis=0)\n",
    "Weights=np.array([stratipyCompGDiff[i,idx] for idx,i in enumerate(Stratification)])\n",
    "hist(Weights,200)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for comp in range(3):\n",
    "    selectedGenes=symbols[((Stratification==comp)*(Weights>0.01))]\n",
    "    print comp+1,len(selectedGenes)\n",
    "    for g in selectedGenes:\n",
    "        print g[0][0]\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp=[k for i,k in enumerate(good+bad) if degree[i]>0]\n",
    "selectedGenes=[k for i,k in enumerate(tmp) if notAlone[i]]\n",
    "\n",
    "df0=pd.DataFrame({'EntrezId':[g[0] for g in data['gene_id_all'][selectedGenes]],'Genes':[g[0][0] for g in data['gene_id_symbol'][selectedGenes]]})\n",
    "df1=pd.DataFrame({'StartiPyDiff_1':stratipyCompGDiff[0,:].T,'StartiPyDiff_2':stratipyCompGDiff[1,:].T,'StartiPyDiff_3':stratipyCompGDiff[2,:].T,'StratiPyDiff_W':stratipyCompGDiff.sum(axis=0).T,'StratiPyDiff_Comp':np.argmax(stratipyCompGDiff, axis=0).T})\n",
    "df2=pd.DataFrame({'StartiPy_1':stratipyCompG[0,:].T,'StartiPy_2':stratipyCompG[1,:].T,'StartiPy_3':stratipyCompG[2,:].T,'StratiPy_W':stratipyCompG.sum(axis=0).T,'StratiPy_Comp':np.argmax(stratipyCompG, axis=0).T})\n",
    "df3=pd.DataFrame({'NNF_1':sklearnComp[0,:].T,'NNF_2':sklearnComp[1,:].T,'NNF_3':sklearnComp[2,:].T,'NNF_W':sklearnComp.sum(axis=0).T,'NNF_Comp':np.argmax(sklearnComp, axis=0).T})\n",
    "df4=pd.DataFrame({'NNFDiff_1':sklearnCompDiff[0,:].T,'NNFDiff_2':sklearnCompDiff[1,:].T,'NNFDiff_3':sklearnCompDiff[2,:].T,'NNFDiff_W':sklearnCompDiff.sum(axis=0).T,'NNFDiff_Comp':np.argmax(sklearnCompDiff, axis=0).T})\n",
    "pd.concat([df0,df1,df2,df3,df4],axis=1).to_csv(dataFolder+'StratificationResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "H=nx.from_numpy_matrix(np.matrix(nnnetFiltered))\n",
    "nx.write_edgelist(H, dataFolder+\"Hofree-edgelist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,10))\n",
    "pos=nx.graphviz_layout(H,prog=\"neato\")\n",
    "node_color=np.argmax(stratipyCompGDiff, axis=0)\n",
    "nx.draw(H,pos,with_labels=False,node_size=50,node_color=node_color,cmap = plt.cm.Pastel1)\n",
    "cut = 1.05\n",
    "xmax= cut*max(xx for xx,yy in pos.values())\n",
    "ymax= cut*max(yy for xx,yy in pos.values())\n",
    "plt.xlim(0,xmax)\n",
    "plt.ylim(0,ymax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the effects of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err=np.zeros((20,11))\n",
    "for ncomp in range(20):\n",
    "    for smooth in range(11):\n",
    "        print \"Ncomp=\",ncomp+1,\" Smooth=\",smooth/10.,\n",
    "        WDiff2,stratipyCompGDiff2,error = GNMF(np.matrix(nnmutDiffused),np.matrix(nnnetFiltered),smooth/10.,n_components=ncomp+1,tol=1e-3,max_iter=5)\n",
    "        err[ncomp,smooth]=error\n",
    "        print \" Error=\",error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err2=np.zeros((20,11))\n",
    "for ncomp in range(20):\n",
    "    for smooth in range(11):\n",
    "        print \"Ncomp=\",ncomp+1,\" Smooth=\",smooth/10.,\n",
    "        WDiff2,stratipyCompGDiff2,error = GNMF(np.matrix(nnmutFiltered),np.matrix(nnnetFiltered),smooth/10.,n_components=ncomp+1,tol=1e-3,max_iter=5)\n",
    "        err2[ncomp,smooth]=error\n",
    "        print \" Error=\",error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(20,11))\n",
    "subplot(121)\n",
    "imshow(err, interpolation=\"nearest\")\n",
    "gca().invert_yaxis()\n",
    "xticks(np.arange(11),np.arange(11)/10.)\n",
    "yticks(np.arange(20),np.arange(20)+1)\n",
    "ylabel(\"Number of Component(s)\")\n",
    "xlabel(\"Smoothing factor\")\n",
    "title(\"Absolute error\")\n",
    "colorbar()\n",
    "subplot(122)\n",
    "imshow(err-np.matrix(mean(err,axis=1)).T*np.matrix(np.ones(11)), interpolation=\"nearest\")\n",
    "gca().invert_yaxis()\n",
    "xticks(np.arange(11),np.arange(11)/10.)\n",
    "yticks(np.arange(20),np.arange(20)+1)\n",
    "ylabel(\"Number of Component(s)\")\n",
    "xlabel(\"Smoothing factor\")\n",
    "title(\"Relative error by number of component\")\n",
    "colorbar()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(20,10))\n",
    "subplot(121)\n",
    "imshow(err2, interpolation=\"nearest\")\n",
    "gca().invert_yaxis()\n",
    "xticks(np.arange(11),np.arange(11)/10.)\n",
    "yticks(np.arange(20),np.arange(20)+1)\n",
    "ylabel(\"Number of Component(s)\")\n",
    "xlabel(\"Smoothing factor\")\n",
    "title(\"Absolute error\")\n",
    "colorbar()\n",
    "subplot(122)\n",
    "imshow(err2-np.matrix(mean(err2,axis=1)).T*np.matrix(np.ones(11)), interpolation=\"nearest\")\n",
    "gca().invert_yaxis()\n",
    "xticks(np.arange(11),np.arange(11)/10.)\n",
    "yticks(np.arange(20),np.arange(20)+1)\n",
    "ylabel(\"Number of Component(s)\")\n",
    "xlabel(\"Smoothing factor\")\n",
    "title(\"Relative error by number of component\")\n",
    "colorbar()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(16,10))\n",
    "subplot(121)\n",
    "plot(np.vstack((err.mean(axis=1)-err.mean(),err2.mean(axis=1)-err2.mean())).T)\n",
    "ylabel(\"Average relative reconstruction error\")\n",
    "xlabel(\"Number of Component(s)\")\n",
    "subplot(122)\n",
    "plot(np.vstack((err.mean(axis=0)-err.mean(),err2.mean(axis=0)-err2.mean())).T)\n",
    "ylabel(\"Average relative reconstruction error\")\n",
    "xlabel(\"Smoothing factor\")\n",
    "legend({\"Diffused\",\"Filtered\"})\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under construction ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Homemade PPI ...\n",
    "import pandas as pd\n",
    "from os.path import isfile, join, expanduser\n",
    "mypath=expanduser(\"~/Dropbox/science/Pasteur/Listes Genes/\")\n",
    "SelectedPPI=pd.read_csv(join(mypath,'PPI_150526.csv'))\n",
    "SelectedPPI.head()\n",
    "G=nx.from_pandas_dataframe(SelectedPPI, 'entrez_gene_ida', 'entrez_gene_idb')\n",
    "G=G.to_undirected()\n",
    "nodelist=[g[0] for g in data['gene_id_all'][good+bad]]\n",
    "nnnnet=nx.to_numpy_matrix(G,nodelist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
